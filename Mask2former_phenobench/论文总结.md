# Mask2former论文学习笔记

***

## 一.预备知识

由于Mask2fomer论文中提及了许多模式识别课程之外的模型或者知识，在阅读的过程中我遇到了很多困难，于是我先去学习了一下额外的预备知识。

### 1.Transformer

Transformer是一个基于注意力机制和Encoder-Decoder的网络主干结构，最早用于机器翻译，后来研究者发现其在自然语言处理和图像分类中取得了优越的性能。其基本形式适用于序列建模任务。

#### Encoder-Decoder

* Encoder-Decoder框架顾名思义也就是编码-解码框架，目前大部分attention模型都是依附于Encoder-Decoder框架进行实现。
* 编码器对于输入的序列<x1,x2,x3…xn>进行编码，使其转化为一个语义编码C，解码器根据输入的语义编码C，然后将其解码成序列数据<y1,y2,y3…yn>，解码方式可以采用RNN/LSTM。
* 普通的Encoder-Decoder结构存在一定的问题：解码器在生成序列数据时，使用的语义编码C是相同的，这意味着不论是生成哪个序列数据，y1,y2还是y3，其实输入序列X中任意数据对生成某个目标数据yi来说影响力都是相同的，没有任何区别。另外一个问题是：如果输入序列一长，只是用一个语义编码C来表示整个序列的信息肯定会损失很多信息，这样将所有信息压缩在一个C里面显然就不合理。

#### Attention model

* Attention model模仿人的注意力机制。包括Query(Q),key(k),value(V)三个部分。V相当于一个知识库，是我们手头已有的资料，K是知识库的钥匙(标志?)，Q是我们待查询的东西。
* 先将Q和K做点积查看两者的相似度关系，接着使用softmax函数进行归一化，得到attention矩阵，将attetion矩阵与V的乘积作为输出。\[O = softmax(Q{K^T})V\]
* Transformer中还对上述注意力机制进行了改进，使用了“多头注意力机制”，多头注意力机制假设输入的特征空间可以分为互不相交的几个子空间，然后我们只需要在几个子空间内单独计算注意力，最后将多个子空间的计算结果拼接即可。

#### Padding Mask

* 简单总结来说就是输入序列长度不一致时需要统一batch，当对较短的序列进行padding时，引入掩膜机制，将序列padding对应的attention权重赋值为0，让attention的计算结果不受padding的影响。

#### position encoding（位置编码）

* 在Transformer中，模型输入其实隐含了前后顺序，然而，从attention的形式看，attention并不具备上述对位置进行编码的能力，这是因为attention对所有输入都一视同仁，因此，为了保留原始输入的顺序关系，我们需要在Transformer的输入中加入表征顺序的特征，进行位置编码。

#### Transformer原理

![transformer](https://raw.githubusercontent.com/XB304/image/main/img/transformer.png)

* 如图所示，transformer由编码器和解码器组成，解码器包含掩膜多注意力模块，归一化模块，前馈全连接神经网络，输出经线性化后经softmax层，输出最为可能的预测值。

## 2.Maskformer

作为Mask2former模型的前身，在阅读论文前首先对Maskformer进行了一定的了解。

![maskformer](https://raw.githubusercontent.com/XB304/image/main/img/maskformer.png)

## 3.图像分割任务

* 语义分割(semantic segmentation)：对图像中的每个像素打上类别标签。
* 实例分割(instance segmentation)：目标检测和语义分割的结合，在图像中将目标检测出来，然后对对图像中的每个像素打上类别标签进行分类（语义分割）。
* 全景分割(Panoptic segmentation)：全景分割将语义分割（为每个像素分配一个类标签）和实例分割（检测并分割每个对象实例）的典型任务统一起来。

***

## 二.论文阅读

### 1.模型的提出背景及优越性

图像分割任务可分为全景分割，语义分割和实例分割，针对专门任务的专用架构如Max-DeepLab和BEiT等虽然具有较好的性能，但是这些模型只能针对特定分割领域的具体任务，不具有灵活性和普适性。基于DETR的通用架构Maskformer不仅在全景分割上表现良好，语义分割也达到了先进水平，K-net将通用架构拓展到了实例分割。但是这些通用架构相比专用架构需要更好的硬件和更长的训练时间，在性能上不如专用架构。

Masked-attention Mask Trans-former (Mask2Former)的提出优化了Maskformer的性能，Mask2former是第一个在所有考虑的任务和数据集上都优于最先进的专用架构的架构。

### 2.模型架构
